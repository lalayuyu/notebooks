{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pix2Voxel论文的notebook版本"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 配置文件"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from easydict import EasyDict as eDict\n",
    "\n",
    "__C = eDict()\n",
    "cfg = __C\n",
    "\n",
    "#数据集设置\n",
    "__C.DATA_CONFIG = eDict()\n",
    "__C.DATA_CONFIG.PIX3D = eDict()\n",
    "__C.DATA_CONFIG.PIX3D.TAXONOMY_FILE_PATH = './DataSets/Pix3D.json'\n",
    "__C.DATA_CONFIG.PIX3D.ANNOTATION_PATH = './DataSets/PiX3D/pix3d.json'\n",
    "__C.DATA_CONFIG.PIX3D.RENDERING_PATH = './DataSets/PiX3D/img/%s/%s.%s'\n",
    "__C.DATA_CONFIG.PIX3D.VOXEL_PATH = './DataSets/PiX3D/model/%s/%s/%s.binvox'\n",
    "\n",
    "__C.DATASET = eDict()\n",
    "__C.DATASET.MEAN = [0.5, 0.5, 0.5]\n",
    "__C.DATASET.STD = [0.5, 0.5, 0.5]\n",
    "__C.DATASET.TRAIN_DATASET = 'Pix3D'\n",
    "__C.DATASET.TEST_DATASET = 'Pix3D'\n",
    "\n",
    "# 文件路径配置\n",
    "__C.DIR = eDict()\n",
    "__C.DIR.OUT_PATH = './output'\n",
    "\n",
    "# 神经网络配置\n",
    "__C.NETWORK = eDict()\n",
    "__C.NETWORK.LEAKY_VALUE = .2\n",
    "__C.NETWORK.CONV_USE_BIAS = False\n",
    "__C.NETWORK.USE_REFINER = True\n",
    "__C.NETWORK.USE_MERGER = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据集的预处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __call__(self, rendering_images, bounding_box=None):\n",
    "        for t in self.transforms:\n",
    "            if t.__class__.__name__ == 'RandomCrop' or t.__class__.__name__ == 'CenterCrop':\n",
    "                rendering_images = t(rendering_images, bounding_box)\n",
    "            else:\n",
    "                rendering_images = t(rendering_images)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 神经网络模块的定义，暂时不使用refiner"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, config1):\n",
    "        super().__init__()\n",
    "        self.cfg = config1\n",
    "\n",
    "        # 网络结构定义\n",
    "        vgg16_bn = models.vgg16_bn(pretrained=True)\n",
    "        self.vgg = torch.nn.Sequential(*list(vgg16_bn.features.children()))[:27]\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(512, 512, kernel_size=3),\n",
    "            torch.nn.BatchNorm2d(512),\n",
    "            torch.nn.ELU()\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(512, 512, kernel_size=3),\n",
    "            torch.nn.BatchNorm2d(512),\n",
    "            torch.nn.ELU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=3)\n",
    "        )\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(512, 256, kernel_size=1),\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.ELU()\n",
    "        )\n",
    "\n",
    "        for param in vgg16_bn.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, rendering_images):\n",
    "        rendering_images = rendering_images.permute(1, 0, 2, 3, 4).contiguous()\n",
    "        rendering_images = torch.split(rendering_images, 1 ,dim=0)\n",
    "        image_features = []\n",
    "\n",
    "        for img in rendering_images:\n",
    "            features = self.vgg(img.squeeze(dim=0))\n",
    "            features = self.layer1(features)\n",
    "            features = self.layer2(features)\n",
    "            features = self.layer3(features)\n",
    "            image_features.append(features)\n",
    "            \n",
    "        image_features = torch.stack(image_features).permute(1, 0, 2, 3, 4).contiguous()\n",
    "        print(image_features.size())\n",
    "        return image_features\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 解码器构建"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, config2):\n",
    "        super().__init__()\n",
    "        self.config2 = config2\n",
    "        \n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "                torch.nn.ConvTranspose3d(2048, 512, kernel_size=4, stride=2, bias=cfg.NETWORK.TCONV_USE_BIAS, padding=1),\n",
    "                torch.nn.BatchNorm3d(512),\n",
    "                torch.nn.ReLU()\n",
    "            )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose3d(512, 128, kernel_size=4, stride=2, bias=cfg.NETWORK.TCONV_USE_BIAS, padding=1),\n",
    "            torch.nn.BatchNorm3d(128),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose3d(128, 32, kernel_size=4, stride=2, bias=cfg.NETWORK.TCONV_USE_BIAS, padding=1),\n",
    "            torch.nn.BatchNorm3d(32),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose3d(32, 8, kernel_size=4, stride=2, bias=cfg.NETWORK.TCONV_USE_BIAS, padding=1),\n",
    "            torch.nn.BatchNorm3d(8),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.layer5 = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose3d(8, 1, kernel_size=1, bias=cfg.NETWORK.TCONV_USE_BIAS),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, image_features):\n",
    "        image_features = image_features.permute(1, 0, 2, 3, 4).contiguous()\n",
    "        image_features = torch.split(image_features, 1, dim=0)\n",
    "        gen_volumes = []\n",
    "        raw_features = []\n",
    "\n",
    "        for features in image_features:\n",
    "            gen_volume = features.view(-1, 2048, 2, 2, 2)\n",
    "            # print(gen_volume.size())   # torch.Size([batch_size, 2048, 2, 2, 2])\n",
    "            gen_volume = self.layer1(gen_volume)\n",
    "            # print(gen_volume.size())   # torch.Size([batch_size, 512, 4, 4, 4])\n",
    "            gen_volume = self.layer2(gen_volume)\n",
    "            # print(gen_volume.size())   # torch.Size([batch_size, 128, 8, 8, 8])\n",
    "            gen_volume = self.layer3(gen_volume)\n",
    "            # print(gen_volume.size())   # torch.Size([batch_size, 32, 16, 16, 16])\n",
    "            gen_volume = self.layer4(gen_volume)\n",
    "            raw_feature = gen_volume\n",
    "            # print(gen_volume.size())   # torch.Size([batch_size, 8, 32, 32, 32])\n",
    "            gen_volume = self.layer5(gen_volume)\n",
    "            # print(gen_volume.size())   # torch.Size([batch_size, 1, 32, 32, 32])\n",
    "            raw_feature = torch.cat((raw_feature, gen_volume), dim=1)\n",
    "            # print(raw_feature.size())  # torch.Size([batch_size, 9, 32, 32, 32])\n",
    "\n",
    "            gen_volumes.append(torch.squeeze(gen_volume, dim=1))\n",
    "            raw_features.append(raw_feature)\n",
    "\n",
    "        gen_volumes = torch.stack(gen_volumes).permute(1, 0, 2, 3, 4).contiguous()\n",
    "        raw_features = torch.stack(raw_features).permute(1, 0, 2, 3, 4, 5).contiguous()\n",
    "        # print(gen_volumes.size())      # torch.Size([batch_size, n_views, 32, 32, 32])\n",
    "        # print(raw_features.size())     # torch.Size([batch_size, n_views, 9, 32, 32, 32])\n",
    "        return raw_features, gen_volumes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##融合模块"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Merger(torch.nn.Module):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}